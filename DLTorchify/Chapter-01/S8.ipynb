{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using convolutions to generalize\n",
    "- Understanding convolution\n",
    "- Creating custom `nn.Module` subclasses\n",
    "- difference between the `nn.module` and `nn.functional` APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = r'C:\\Users\\muthu\\GitHub\\DATA üìÅ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: C:\\Users\\muthu\\GitHub\\DATA üìÅ\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.49139968, 0.48215827, 0.44653124), std=(0.24703233, 0.24348505, 0.26158768))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "CIFAR10:torchvision.datasets.CIFAR10  = torchvision.datasets.CIFAR10(\n",
    "                                                    root=DATAPATH,\n",
    "                                                    download=False,\n",
    "                                                    transform=torchvision.transforms.Compose([\n",
    "                                                            torchvision.transforms.ToTensor(),\n",
    "                                                            torchvision.transforms.Normalize(\n",
    "                                                                mean=(0.49139968, 0.48215827 ,0.44653124),\n",
    "                                                                std=(0.24703233 ,0.24348505, 0.26158768))\n",
    "                                                    ])    \n",
    ")\n",
    "\n",
    "print(CIFAR10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What convolutions do\n",
    "- Convolution, or more precisely, discrete convolution localized patterns to have an effect on the output regardless of their location in the image: that is, to be *translation invariant*\n",
    "- typically we use small kernels ( optimized for GPU and stacking kernels will result Respective Field Steadly! )\n",
    "- since it's small parameter model with lot fewer parameters\n",
    "- `nn.Conv2d` expects a *( Batch x Channel x Height x Width )* shape as tensor\n",
    "\n",
    "<div align='center'> <img src=\"../assets/locality%20and%20translation%20invariance.png\" alt=\"convolution\" height=500 width=1000></img> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Padding, and Maxpooling\n",
    "##### padding \n",
    "- helps in size of an image adjustments (U-Net / Residual Connection) \n",
    "- separate the matter of convolution (changing image sizes) by creating *ghost* pixels around the border\n",
    "\n",
    "<div align='center'> <img src=\"../assets/padding.png\" alt=\"convolution\" height=500 width=1000 />  </div>\n",
    "\n",
    "##### MaxPooling\n",
    "combining convolutions and downsampling for great good and increase in RF\n",
    "<div align='center'>  <img src=\"../assets/maxpool.png\" alt=\"maxpool\" height=500 width=1000 /> </div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Detecting features with convoltuions\n",
    "\n",
    "<div align='center'> <img src=\"../assets/learning%20with%20conv.png\" height=500 width=1000></img> </div>\n",
    "\n",
    "In fact, the job of a computer vision expert has historically been to come up with the most effective combination of filters so that certain features are highlighted in images and objects can be recognized.\n",
    "\n",
    "Here we let model to figure it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32]) torch.Size([1, 16, 30, 30])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conv = torch.nn.Conv2d(3,16,kernel_size=3,padding=0,stride=1,bias=True)\n",
    "\n",
    "## Average Kernel (Blurr)\n",
    "with torch.no_grad():\n",
    "    conv.bias.zero_()\n",
    "with torch.no_grad():\n",
    "    conv.weight.fill_(1./4.0)\n",
    "\n",
    "##  Edge-Detection Kernel \n",
    "# with torch.no_grad():\n",
    "#     conv.weight[:] = torch.tensor([[-1.0, 0.0, 1.0], [-1.0, 0.0, 1.0],[-1.0, 0.0, 1.0]])\n",
    "\n",
    "\n",
    "img, label = CIFAR10[2]\n",
    "output     = conv(img.unsqueeze(0))\n",
    "print(img.unsqueeze(0).shape, output.shape)\n",
    "\n",
    "plt.imshow(output[0,0].detach().cpu().numpy(),cmap='gray')\n",
    "plt.title(CIFAR10.classes[label])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "```python\n",
    " model = nn.Sequential(\n",
    "                torch.torch.torch.torch.nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "                torch.torch.nn.Tanh(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "                nn.Tanh(),\n",
    "                nn.MaxPool2d(2),\n",
    "                # ...\n",
    "                nn.Linear(8 * 8 * 8, 32),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(32, 2))\n",
    "```\n",
    "<div align='center'> \n",
    "    <img src=\"../assets/typical%20cnn%20architecture.png\" alt=\"typical cnn architecture\" height=500 width=1000>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subclassing `nn.Module`\n",
    "-  How PyTorch keeps track of parameters and submodules\n",
    "\n",
    "In order to subclass `nn.Module` at minimum we need to define `forward` function that takes the inputs to the module and returns the output. backward propagation will take care by autograd automatically.\n",
    " \n",
    "`Note`\n",
    "you need to call `super().__init__()` before you can do that (or PyTorch will remind you)\n",
    "\n",
    "The submodules must be top-level attributes, not buried inside list or dict instance. Otherwise optimizer will not able to locate the submodules use `nn.ModuleList` and `nn.ModuleDict`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels=3,out_channels=2):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels,16,kernel_size=3,padding=1)\n",
    "        self.act1  = torch.nn.Tanh()\n",
    "        self.pool1 = torch.nn.MaxPool2d(2)\n",
    "        self.conv2 = torch.nn.Conv2d(16,8,kernel_size=3,padding=1)\n",
    "        self.act2  = torch.nn.Tanh()\n",
    "        self.pool2 = torch.nn.MaxPool2d(2)\n",
    "        self.fc1   = torch.nn.Linear(8*8*8,32)\n",
    "        self.act3  = torch.nn.Tanh()\n",
    "        self.fc2   = torch.nn.Linear(32,out_channels) \n",
    "\n",
    "    def forward(self,x, *args, **kwargs):\n",
    "        out = self.pool1( self.act1( self.conv1(x) ) )\n",
    "        out = self.pool2( self.act2( self.conv2(out) ) )\n",
    "        out = out.view(-1,8*8*8)\n",
    "        out = self.fc2( self.act3(self.fc1(out)) )\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters:: 18090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "====================================================================================================================================================================================\n",
       "Layer (type (var_name):depth-idx)        Input Shape          Output Shape         Param #              Param %              Kernel Shape         Mult-Adds            Trainable\n",
       "====================================================================================================================================================================================\n",
       "Net (Net)                                [1, 3, 32, 32]       [1, 2]               --                        --              --                   --                   True\n",
       "‚îú‚îÄConv2d (conv1): 1-1                    [1, 3, 32, 32]       [1, 16, 32, 32]      448                    2.48%              [3, 3]               458,752              True\n",
       "‚îú‚îÄTanh (act1): 1-2                       [1, 16, 32, 32]      [1, 16, 32, 32]      --                        --              --                   --                   --\n",
       "‚îú‚îÄMaxPool2d (pool1): 1-3                 [1, 16, 32, 32]      [1, 16, 16, 16]      --                        --              2                    --                   --\n",
       "‚îú‚îÄConv2d (conv2): 1-4                    [1, 16, 16, 16]      [1, 8, 16, 16]       1,160                  6.41%              [3, 3]               296,960              True\n",
       "‚îú‚îÄTanh (act2): 1-5                       [1, 8, 16, 16]       [1, 8, 16, 16]       --                        --              --                   --                   --\n",
       "‚îú‚îÄMaxPool2d (pool2): 1-6                 [1, 8, 16, 16]       [1, 8, 8, 8]         --                        --              2                    --                   --\n",
       "‚îú‚îÄLinear (fc1): 1-7                      [1, 512]             [1, 32]              16,416                90.75%              --                   16,416               True\n",
       "‚îú‚îÄTanh (act3): 1-8                       [1, 32]              [1, 32]              --                        --              --                   --                   --\n",
       "‚îú‚îÄLinear (fc2): 1-9                      [1, 32]              [1, 2]               66                     0.36%              --                   66                   True\n",
       "====================================================================================================================================================================================\n",
       "Total params: 18,090\n",
       "Trainable params: 18,090\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.77\n",
       "====================================================================================================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.15\n",
       "Params size (MB): 0.07\n",
       "Estimated Total Size (MB): 0.23\n",
       "===================================================================================================================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net(in_channels=3,out_channels=2)\n",
    "print(f\"number of parameters:: {sum(p.numel() for p in model.parameters() )}\")\n",
    "\n",
    "summary(\n",
    "        model=model, \n",
    "        input_size=(1,3,32,32),\n",
    "        col_width=20,\n",
    "        row_settings=[\"depth\",\"var_names\"],\n",
    "        col_names=[\"input_size\",\"output_size\",\"num_params\",\"params_percent\", \"kernel_size\",\"mult_adds\",\"trainable\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The functional API `nn.functional`\n",
    "\n",
    "By \"functional\" here we mean \"having no internal state\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters:: 18090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "====================================================================================================================================================================================\n",
       "Layer (type (var_name):depth-idx)        Input Shape          Output Shape         Param #              Param %              Kernel Shape         Mult-Adds            Trainable\n",
       "====================================================================================================================================================================================\n",
       "NetF (NetF)                              [1, 3, 32, 32]       [1, 2]               --                        --              --                   --                   True\n",
       "‚îú‚îÄConv2d (conv1): 1-1                    [1, 3, 32, 32]       [1, 16, 32, 32]      448                    2.48%              [3, 3]               458,752              True\n",
       "‚îú‚îÄConv2d (conv2): 1-2                    [1, 16, 16, 16]      [1, 8, 16, 16]       1,160                  6.41%              [3, 3]               296,960              True\n",
       "‚îú‚îÄLinear (fc1): 1-3                      [1, 512]             [1, 32]              16,416                90.75%              --                   16,416               True\n",
       "‚îú‚îÄLinear (fc2): 1-4                      [1, 32]              [1, 2]               66                     0.36%              --                   66                   True\n",
       "====================================================================================================================================================================================\n",
       "Total params: 18,090\n",
       "Trainable params: 18,090\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.77\n",
       "====================================================================================================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.15\n",
       "Params size (MB): 0.07\n",
       "Estimated Total Size (MB): 0.23\n",
       "===================================================================================================================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "class NetF(torch.nn.Module):\n",
    "    def __init__(self,in_channels=3,out_channels=2):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = torch.nn.Linear(8 * 8 * 8, 32)\n",
    "        self.fc2 = torch.nn.Linear(32, out_channels)\n",
    "    def forward(self,x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)),2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)),2)\n",
    "        out = out.view(-1,8*8*8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "model = NetF(in_channels=3,out_channels=2)\n",
    "print(f\"number of parameters:: {sum(p.numel() for p in model.parameters() )}\")\n",
    "\n",
    "summary(\n",
    "        model=model, \n",
    "        input_size=(1,3,32,32),\n",
    "        col_width=20,\n",
    "        row_settings=[\"depth\",\"var_names\"],\n",
    "        col_names=[\"input_size\",\"output_size\",\"num_params\",\"params_percent\", \"kernel_size\",\"mult_adds\",\"trainable\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later,  we will briefly touch on quantization. Then stateless bits like activations suddenly become stateful because information about the quantization needs to be captured. This means if we aim to quantize our model, it might be worthwhile to stick with the modular API if we go for non-JITed quantization\n",
    "\n",
    "\n",
    "Recall that the core of our convnet is two nested loops: an outer one over the `epochs` and an inner one of the DataLoader that produces `batches` from our Dataset.\n",
    "\n",
    "In each loop, we then have to\n",
    " - Feed the inputs through the model (the forward pass).\n",
    " - Compute the loss (also part of the forward pass).\n",
    " - Zero any old gradients.\n",
    " - Call loss.backward() to compute the gradients of the loss with respect to all parameters (the backward pass).\n",
    " - Have the optimizer take a step in toward lower loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model design\n",
    "\n",
    "### Adding memory capacity: Width\n",
    "- First dimension is **Width** of the network: number of channels per convolution\n",
    "\n",
    "### Helping our model to converge and generalize: Regualarization and Dropout\n",
    "- **L1** and **L2** norm\n",
    "- dropout should normally active when model training.\n",
    "\n",
    "\n",
    "### KEEPING ACTIVATIONS IN CHECK: BATCH NORMALIZATION\n",
    "- Reducing the internal convariate sift (address different features present in different channels)\n",
    "\n",
    "### Going deeper to learn more complex structures: Depth (Residual Connection)\n",
    "- Since this is a deep learning, depth is something we‚Äôre supposedly into.\n",
    "-  Depth allows a model to deal with hierarchical information when we need to understand the context in order to say something about some input.\n",
    "- A skip connection is nothing but the addition of the input to the output of a block of layers.\n",
    "<div align='center'>\n",
    "    <img src=\"../assets/residual connection.png\" height=600 width=300  alt='residual'/>\n",
    "</div>\n",
    "\n",
    "we're using the output of the first activations as inputs to the last, in addition to standard feed-forward path. This is referred as `identity mapping`. It's not new Highway networks and U-Net made use of skip connections of one-form or another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetRes(torch.nn.Module):\n",
    "  def __init__(self, n_chans1:int=32):\n",
    "    super().__init__()\n",
    "    self.n_chans1 = n_chans1\n",
    "    self.conv1 = torch.nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "    self.conv2 = torch.nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "    self.conv3 = torch.nn.Conv2d(n_chans1 // 2, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "    self.fc1 = torch.nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
    "    self.fc2 = torch.nn.Linear(32, 2)\n",
    "  def forward(self, x):\n",
    "    out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "    out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "    out1 = out\n",
    "    out = F.max_pool2d(torch.relu(self.conv3(out)) + out1, 2)\n",
    "    out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
    "    out = torch.relu(self.fc1(out))\n",
    "    out = self.fc2(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================================================================================================\n",
       "Layer (type (var_name):depth-idx)        Input Shape          Output Shape         Param #              Param %              Kernel Shape         Mult-Adds            Trainable\n",
       "====================================================================================================================================================================================\n",
       "NetRes (NetRes)                          [3, 32, 32]          [1, 2]               --                        --              --                   --                   True\n",
       "‚îú‚îÄConv2d (conv1): 1-1                    [3, 32, 32]          [32, 32, 32]         896                    5.55%              [3, 3]               917,504              True\n",
       "‚îú‚îÄConv2d (conv2): 1-2                    [32, 16, 16]         [16, 16, 16]         4,624                 28.67%              [3, 3]               1,183,744            True\n",
       "‚îú‚îÄConv2d (conv3): 1-3                    [16, 8, 8]           [16, 8, 8]           2,320                 14.38%              [3, 3]               296,960              True\n",
       "‚îú‚îÄLinear (fc1): 1-4                      [1, 256]             [1, 32]              8,224                 50.99%              --                   8,224                True\n",
       "‚îú‚îÄLinear (fc2): 1-5                      [1, 32]              [1, 2]               66                     0.41%              --                   66                   True\n",
       "====================================================================================================================================================================================\n",
       "Total params: 16,130\n",
       "Trainable params: 16,130\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 2.41\n",
       "====================================================================================================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.30\n",
       "Params size (MB): 0.06\n",
       "Estimated Total Size (MB): 0.38\n",
       "===================================================================================================================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelF = NetRes(n_chans1=32)\n",
    "summary(model=modelF,input_size=(3,32,32),col_width=20,\n",
    "        row_settings=[\"depth\",\"var_names\"],\n",
    "        col_names=[\"input_size\",\"output_size\",\"num_params\",\"params_percent\", \"kernel_size\",\"mult_adds\",\"trainable\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>Deep Network</h1>\n",
    "<div align='center'>\n",
    "    <img src=\"../assets/deep nn.png\" alt='deep nn' height=700 width=500></img>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(torch.nn.Module):\n",
    "  def __init__(self, n_chans:int):\n",
    "    super(ResBlock, self).__init__()\n",
    "    self.conv = torch.nn.Conv2d(n_chans, n_chans, kernel_size=3, padding=1, bias=False)\n",
    "    self.batch_norm = torch.nn.BatchNorm2d(num_features=n_chans)\n",
    "    torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu')\n",
    "    torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
    "    torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    out = self.conv(x)\n",
    "    out = self.batch_norm(out)\n",
    "    out = torch.relu(out)\n",
    "    return out + x\n",
    "\n",
    "class NetResDeep(torch.nn.Module):\n",
    "  def __init__(self, n_chans1=32, n_blocks=1):\n",
    "    super().__init__()\n",
    "    self.n_chans1 = n_chans1\n",
    "    self.conv1 = torch.nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "    self.resblocks = torch.nn.Sequential(\n",
    "                *( n_blocks* [ResBlock(n_chans=n_chans1)] )\n",
    "    )\n",
    "    self.fc1 = torch.nn.Linear(8 * 8 * n_chans1, 32)\n",
    "    self.fc2 = torch.nn.Linear(32, 2)\n",
    "  def forward(self, x):\n",
    "    out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "    out = self.resblocks(out)\n",
    "    out = F.max_pool2d(out, 2)\n",
    "    out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "    out = torch.relu(self.fc1(out))\n",
    "    out = self.fc2(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================================================================================================\n",
       "Layer (type (var_name):depth-idx)                  Input Shape          Output Shape         Param #              Param %              Kernel Shape         Mult-Adds            Trainable\n",
       "==============================================================================================================================================================================================\n",
       "NetResDeep (NetResDeep)                            [1, 3, 32, 32]       [1, 2]               --                        --              --                   --                   True\n",
       "‚îú‚îÄConv2d (conv1): 1-1                              [1, 3, 32, 32]       [1, 32, 32, 32]      896                    1.18%              [3, 3]               917,504              True\n",
       "‚îú‚îÄSequential (resblocks): 1-2                      [1, 32, 16, 16]      [1, 32, 16, 16]      --                        --              --                   --                   True\n",
       "‚îÇ    ‚îî‚îÄResBlock (4): 2-1                           [1, 32, 16, 16]      [1, 32, 16, 16]      --                        --              --                   --                   True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d (conv): 3-1                     [1, 32, 16, 16]      [1, 32, 16, 16]      9,216                 12.16%              [3, 3]               2,359,296            True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d (batch_norm): 3-2          [1, 32, 16, 16]      [1, 32, 16, 16]      64                     0.08%              --                   64                   True\n",
       "‚îÇ    ‚îî‚îÄResBlock (4): 2-2                           [1, 32, 16, 16]      [1, 32, 16, 16]      (recursive)          (recursive)          --                   --                   True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d (conv): 3-3                     [1, 32, 16, 16]      [1, 32, 16, 16]      (recursive)          (recursive)          [3, 3]               2,359,296            True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d (batch_norm): 3-4          [1, 32, 16, 16]      [1, 32, 16, 16]      (recursive)          (recursive)          --                   64                   True\n",
       "‚îÇ    ‚îî‚îÄResBlock (4): 2-3                           [1, 32, 16, 16]      [1, 32, 16, 16]      (recursive)          (recursive)          --                   --                   True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d (conv): 3-5                     [1, 32, 16, 16]      [1, 32, 16, 16]      (recursive)          (recursive)          [3, 3]               2,359,296            True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d (batch_norm): 3-6          [1, 32, 16, 16]      [1, 32, 16, 16]      (recursive)          (recursive)          --                   64                   True\n",
       "‚îÇ    ‚îî‚îÄResBlock (4): 2-4                           [1, 32, 16, 16]      [1, 32, 16, 16]      (recursive)          (recursive)          --                   --                   True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d (conv): 3-7                     [1, 32, 16, 16]      [1, 32, 16, 16]      (recursive)          (recursive)          [3, 3]               2,359,296            True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d (batch_norm): 3-8          [1, 32, 16, 16]      [1, 32, 16, 16]      (recursive)          (recursive)          --                   64                   True\n",
       "‚îÇ    ‚îî‚îÄResBlock (4): 2-5                           [1, 32, 16, 16]      [1, 32, 16, 16]      (recursive)          (recursive)          --                   --                   True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d (conv): 3-9                     [1, 32, 16, 16]      [1, 32, 16, 16]      (recursive)          (recursive)          [3, 3]               2,359,296            True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d (batch_norm): 3-10         [1, 32, 16, 16]      [1, 32, 16, 16]      (recursive)          (recursive)          --                   64                   True\n",
       "‚îú‚îÄLinear (fc1): 1-3                                [1, 2048]            [1, 32]              65,568                86.49%              --                   65,568               True\n",
       "‚îú‚îÄLinear (fc2): 1-4                                [1, 32]              [1, 2]               66                     0.09%              --                   66                   True\n",
       "==============================================================================================================================================================================================\n",
       "Total params: 75,810\n",
       "Trainable params: 75,810\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 12.78\n",
       "==============================================================================================================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.92\n",
       "Params size (MB): 0.30\n",
       "Estimated Total Size (MB): 1.23\n",
       "=============================================================================================================================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelDeep = NetResDeep(n_chans1=32,n_blocks=5)\n",
    "summary(\n",
    "    model=modelDeep,\n",
    "    input_size=(1,3,32,32),\n",
    "    col_width=20,\n",
    "    row_settings=[\"depth\",\"var_names\"],\n",
    "    col_names=[\"input_size\",\"output_size\",\"num_params\",\"params_percent\", \"kernel_size\",\"mult_adds\",\"trainable\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(torch.nn.Module):\n",
    "  def __init__(self, n_chans:int):\n",
    "    super(ResBlock, self).__init__()\n",
    "    self.conv = torch.nn.Conv2d(n_chans, n_chans, kernel_size=5, padding=2, bias=False)\n",
    "    self.batch_norm = torch.nn.BatchNorm2d(num_features=n_chans)\n",
    "    torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu')\n",
    "    torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
    "    torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    out = self.conv(x)\n",
    "    out = self.batch_norm(out)\n",
    "    out = torch.relu(out)\n",
    "    return out + x\n",
    "\n",
    "class NetResDeep(torch.nn.Module):\n",
    "  def __init__(self, n_chans1=32, n_blocks=1):\n",
    "    super().__init__()\n",
    "    self.n_chans1 = n_chans1\n",
    "    self.conv1 = torch.nn.Conv2d(3, n_chans1, kernel_size=5, padding=2)\n",
    "    self.resblocks = torch.nn.Sequential(\n",
    "                *( n_blocks* [ResBlock(n_chans=n_chans1)] )\n",
    "    )\n",
    "    self.fc1 = torch.nn.Linear(8 * 8 * n_chans1, 32)\n",
    "    self.fc2 = torch.nn.Linear(32, 2)\n",
    "  def forward(self, x):\n",
    "    out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "    out = self.resblocks(out)\n",
    "    out = F.max_pool2d(out, 2)\n",
    "    out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "    out = torch.relu(self.fc1(out))\n",
    "    out = self.fc2(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================================================================================================\n",
       "Layer (type (var_name):depth-idx)                  Input Shape          Output Shape         Param #              Param %              Kernel Shape         Mult-Adds            Trainable\n",
       "==============================================================================================================================================================================================\n",
       "NetResDeep (NetResDeep)                            [1, 3, 32, 32]       [1, 2]               --                        --              --                   --                   True\n",
       "‚îú‚îÄConv2d (conv1): 1-1                              [1, 3, 32, 32]       [1, 32, 32, 32]      2,432                  2.59%              [5, 5]               2,490,368            True\n",
       "‚îú‚îÄSequential (resblocks): 1-2                      [1, 32, 16, 16]      [1, 32, 16, 16]      --                        --              --                   --                   True\n",
       "‚îÇ    ‚îî‚îÄResBlock (4): 2-1                           [1, 32, 16, 16]      [1, 32, 16, 16]      --                        --              --                   --                   True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d (conv): 3-1                     [1, 32, 16, 16]      [1, 32, 16, 16]      25,600                27.31%              [5, 5]               6,553,600            True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d (batch_norm): 3-2          [1, 32, 16, 16]      [1, 32, 16, 16]      64                     0.07%              --                   64                   True\n",
       "‚îÇ    ‚îî‚îÄResBlock (4): 2-2                           [1, 32, 16, 16]      [1, 32, 16, 16]      (recursive)          (recursive)          --                   --                   True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d (conv): 3-3                     [1, 32, 16, 16]      [1, 32, 16, 16]      (recursive)          (recursive)          [5, 5]               6,553,600            True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d (batch_norm): 3-4          [1, 32, 16, 16]      [1, 32, 16, 16]      (recursive)          (recursive)          --                   64                   True\n",
       "‚îÇ    ‚îî‚îÄResBlock (4): 2-3                           [1, 32, 16, 16]      [1, 32, 16, 16]      (recursive)          (recursive)          --                   --                   True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d (conv): 3-5                     [1, 32, 16, 16]      [1, 32, 16, 16]      (recursive)          (recursive)          [5, 5]               6,553,600            True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d (batch_norm): 3-6          [1, 32, 16, 16]      [1, 32, 16, 16]      (recursive)          (recursive)          --                   64                   True\n",
       "‚îÇ    ‚îî‚îÄResBlock (4): 2-4                           [1, 32, 16, 16]      [1, 32, 16, 16]      (recursive)          (recursive)          --                   --                   True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d (conv): 3-7                     [1, 32, 16, 16]      [1, 32, 16, 16]      (recursive)          (recursive)          [5, 5]               6,553,600            True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d (batch_norm): 3-8          [1, 32, 16, 16]      [1, 32, 16, 16]      (recursive)          (recursive)          --                   64                   True\n",
       "‚îÇ    ‚îî‚îÄResBlock (4): 2-5                           [1, 32, 16, 16]      [1, 32, 16, 16]      (recursive)          (recursive)          --                   --                   True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d (conv): 3-9                     [1, 32, 16, 16]      [1, 32, 16, 16]      (recursive)          (recursive)          [5, 5]               6,553,600            True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d (batch_norm): 3-10         [1, 32, 16, 16]      [1, 32, 16, 16]      (recursive)          (recursive)          --                   64                   True\n",
       "‚îú‚îÄLinear (fc1): 1-3                                [1, 2048]            [1, 32]              65,568                69.95%              --                   65,568               True\n",
       "‚îú‚îÄLinear (fc2): 1-4                                [1, 32]              [1, 2]               66                     0.07%              --                   66                   True\n",
       "==============================================================================================================================================================================================\n",
       "Total params: 93,730\n",
       "Trainable params: 93,730\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 35.32\n",
       "==============================================================================================================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.92\n",
       "Params size (MB): 0.37\n",
       "Estimated Total Size (MB): 1.31\n",
       "=============================================================================================================================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelDeep = NetResDeep(n_chans1=32,n_blocks=5)\n",
    "summary(\n",
    "    model=modelDeep,\n",
    "    input_size=(1,3,32,32),\n",
    "    col_width=20,\n",
    "    row_settings=[\"depth\",\"var_names\"],\n",
    "    col_names=[\"input_size\",\"output_size\",\"num_params\",\"params_percent\", \"kernel_size\",\"mult_adds\",\"trainable\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
