{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 14 07:21:08 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 531.14                 Driver Version: 531.14       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4050 L...  WDDM | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   41C    P8                2W /  N/A|      0MiB /  6141MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2448      C   ...thu\\miniconda3\\envs\\venv\\python.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back in days, in order to tell ones 1Ô∏è‚É£ from zeros0Ô∏è‚É£ in images of handwritten digits, we would come up with a set of filtersüßµ to estimate the direction of edges over the image, and then train a classifier to predict the correct digit given a distribution of edge directions. ‚ö°\n",
    "\n",
    "we often need to inject some prior knowledge\n",
    "\n",
    "What does Deep Learning do? ü§î\n",
    "\n",
    "![WhatDoesDLdo](../assets/What%20does%20DL.png)\n",
    "\n",
    "Why PyTorch? \n",
    "- It's more Pythonic üêç\n",
    "- similar to Numpy + GPU\n",
    "-  allowing a developer to implement complicated models without undue complexity being imposed by the library\n",
    "- Consumed Caffe2 for it's backend.\n",
    "- Added a delayed-execution ‚Äúgraph mode‚Äù runtime called TorchScript\n",
    "\n",
    "`Note`::  PyTorch has the ‚ÄúPy‚Äù as in Python üêç, but there‚Äôs a lot of non-Python code in it.\n",
    " Actually, for performance reasons, most of PyTorch is written in C++ and CUDA üî•\n",
    " (www.geforce.com/hardware/technology/cuda), a C++-like language from NVIDIA\n",
    " that can be compiled to run with massive parallelism on GPUs. There are ways to run\n",
    " PyTorch directly from C++ \n",
    "\n",
    "\n",
    " PyTorch is a library that provides\n",
    " - multidimensional arrays (or) tensors\n",
    " - numerical optimization,  provided natively by tensors by virtue of dispatching through PyTorch‚Äôs autograd engine under the hood (used for physics, rendering, optim, simulation, modeling, etc)\n",
    " - buliding nn are located in `torch.nn`\n",
    "    - cnn\n",
    "    - fc\n",
    "    - loss function\n",
    "    - activation\n",
    "    - lstm\n",
    "    - embedding\n",
    "    - .\n",
    "\n",
    "\n",
    "overview of Project üìΩÔ∏è\n",
    "- data source\n",
    "- convert each sample into something PyTorch tensor Standardized\n",
    "    - torch.utils.data.Dataset\n",
    "- parallel processing, need multiple processes to load our data, in order to assemble them into `batches`  pytorch provides `DataLoader` class\n",
    "- the training loop is implemented as a standard Python for loop. \n",
    "- compare the outputs of our model to the desired output (the targets) using some criterion or loss function.\n",
    "- need an optimizer doing the updates to get better outcomes, and that is what PyTorch offers us in torch.optim\n",
    "- deployment involve putting the model on a server or exporting it to load it to a cloud engine\n",
    "\n",
    "![DLProject](../assets/DLProject.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is cuda available:: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import  pyplot as plt \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "print(f\"is cuda available:: {torch.cuda.is_available()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
